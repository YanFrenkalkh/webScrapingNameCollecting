{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e42598ad",
   "metadata": {},
   "source": [
    "# The code bellow Gathers names and surnames from wikipidia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4df7a58",
   "metadata": {},
   "source": [
    "### The imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cd83fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc58aa2",
   "metadata": {},
   "source": [
    "Whe use bs4 for web Crawling and requests for url gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fbd8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base URLS\n",
    "base_url = 'https://en.wikipedia.org'\n",
    "wiki_url = 'https://en.wikipedia.org/wiki/Category:Given_names'\n",
    "surnames_url = 'https://en.wikipedia.org/wiki/Category:Surnames'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5d2de3",
   "metadata": {},
   "source": [
    "base_url - used to move to the next page in the website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8631d846",
   "metadata": {},
   "source": [
    "wiki_url - first url of the first names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24a86ab",
   "metadata": {},
   "source": [
    "surnames_url - first url of the surnames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc61c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data sets\n",
    "all_names = []\n",
    "base_page_id = []\n",
    "result_tuples = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b2d5a1",
   "metadata": {},
   "source": [
    "Lists used to hold the gathered data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97a54a8",
   "metadata": {},
   "source": [
    "### Name extruction function from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d98c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page pars function\n",
    "def extract_names_from_url(url):\n",
    "    # Get http request\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the page whase succsesfuly opend\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Serch for the div that holds the namse and extruct the names ul\n",
    "        given_names_text = soup.find('div', {'id': 'mw-pages'}).find('div', {'class': 'mw-category mw-category-columns'}).findAll('li')\n",
    "\n",
    "        # Clean the data and stor in memory\n",
    "        for given_name in tqdm(given_names_text):\n",
    "            if given_name != '\\n':\n",
    "                all_names.append(given_name.get_text())\n",
    "\n",
    "        # Extruct the next 'next page'\n",
    "        next_page_text = soup.find('div', {'class': 'mw-category-generated'}).findAll('a')[-1].text\n",
    "\n",
    "        if next_page_text == 'next page':\n",
    "            # Extruct the next page partial url\n",
    "            next_page_partial_url = soup.find('div', {'class': 'mw-category-generated'}).findAll('a')[-1]['href']\n",
    "            # Concar the new next page url\n",
    "            next_page_url = base_url + next_page_partial_url\n",
    "            extract_names_from_url(next_page_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b141a7c2",
   "metadata": {},
   "source": [
    "The function extracts data from the website by the pattern of the data representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f95745d",
   "metadata": {},
   "source": [
    "The function moves recursevly thru the website pages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43fa50b",
   "metadata": {},
   "source": [
    "### Data tupples generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd060cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_set_tupple(page_title):\n",
    "    def get_wikidata_info(page_id):\n",
    "        # Get wikidata url\n",
    "        url = f\"https://www.wikidata.org/wiki/{page_id}\"\n",
    "        html = requests.get(url).content.decode('utf-8')\n",
    "\n",
    "        # Parse the data\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        # Get description and rows\n",
    "        description = soup.find('table').findAll('tr')[1].findAll('td')[1].text\n",
    "        wiki_rows = soup.find('ul', {'class': 'wikibase-sitelinklistview-listview'}).findAll('li', {'class': 'wikibase-sitelinkview'})\n",
    "        for row in tqdm(wiki_rows):\n",
    "            # Get langude and entry\n",
    "            langude = row.findAll('span', {'class': 'wikibase-sitelinkview-siteid'})[0]['title']\n",
    "            wiki_short_lang = row.findAll('span', {'class': 'wikibase-sitelinkview-siteid'})[0].text.replace('wiki', '')\n",
    "            entry = row.find('a').text\n",
    "            result_tuples.append((page_title, page_id, description, langude, wiki_short_lang, entry))\n",
    "\n",
    "    # Construct the API URL\n",
    "    api_url = f'https://en.wikipedia.org/w/api.php?action=query&titles={page_title}&prop=pageprops&format=json'\n",
    "\n",
    "    # Make the API request and convert the response to JSON\n",
    "    response = requests.get(api_url).json()\n",
    "    # Extract the page ID from the JSON response\n",
    "    page = next(iter(response['query']['pages'].values()))\n",
    "    if 'pageprops' in page:\n",
    "        page = next(iter(response['query']['pages'].values()))['pageprops']\n",
    "        # Test if the id exsists in wikibase\n",
    "        if 'wikibase_item' in page:\n",
    "            p_id = next(iter(response['query']['pages'].values()))['pageprops']['wikibase_item']\n",
    "            base_page_id.append(p_id)\n",
    "            get_wikidata_info(p_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46d0a32",
   "metadata": {},
   "source": [
    "The function collects (Label, WikiDate ID, English Description, Language, Wiki Short Lang, Entry) and makes a tupple form the collected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5fc5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the extract_names_from_url function with names url\n",
    "extract_names_from_url(wiki_url)\n",
    "# Remove the rundom cupterd data\n",
    "all_names.remove('Given name')\n",
    "all_names.remove('Template:R from given name')\n",
    "all_names.remove('List of most popular given names')\n",
    "all_names.remove('Onomancy')\n",
    "\n",
    "# Run the extract_names_from_url function with surnames url\n",
    "extract_names_from_url(surnames_url)\n",
    "# Remove the rundom cupterd data\n",
    "all_names.remove('Name blending')\n",
    "all_names.remove('One-name study')\n",
    "all_names.remove('Template:R from surname')\n",
    "all_names.remove('Template:Surname')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c23158",
   "metadata": {},
   "source": [
    "Make list of surnames and first names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad275d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop thru the name list and make the coresponding tupple for eche name\n",
    "for name in tqdm(all_names):\n",
    "    make_data_set_tupple(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7d4e17",
   "metadata": {},
   "source": [
    "Execute the make_data_set_tupple function on eche item in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cd9182",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(result_tuples, columns=['Label', 'WikiDate ID', 'English Description', 'Language', 'Wiki Short Lang', 'Entry'])\n",
    "results_df.to_csv('names.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e30f4d3",
   "metadata": {},
   "source": [
    "Make csv file from the tuples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
